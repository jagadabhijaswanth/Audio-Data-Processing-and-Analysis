{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://thecleverprogrammer.com/2024/12/16/audio-data-processing-and-analysis-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow can be used to develop models for various tasks, including natural language processing, image recognition, handwriting recognition, and different computational-based simulations such as partial differential equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful open-source machine-learning framework developed by Google, that empowers developers to construct and train ML models. It is used to implement machine learning and deep learning applications, for the development and research of fascinating ideas in artificial intelligence. TensorFlow is designed with the Python programming language, which makes it an easily understandable framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\jagad\\tensorflow_datasets\\nsynth\\full\\2.3.3...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8695742e987147bf9e0afd794121cbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea05035a4fbc41539333bad8039d0be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac2634aab274f12a19abc81e69475ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset, info = tfds.load('nsynth', split='train', with_info='True')\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset.take(1):\n",
    "    print(\"Available keys:\")\n",
    "    for key in sample.keys():\n",
    "      print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_nsynth(sample):\n",
    "  audio = sample['audio']\n",
    "  label = sample['pitch']\n",
    "  return audio, label\n",
    "\n",
    "processed_dataset = dataset.map(preprocessed_nsynth)\n",
    "\n",
    "for audio, label in processed_dataset.take(1):\n",
    "  print(f\"Audio Shape: {audio.shape}\")\n",
    "  print(f\"Label (Pitch): {label.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the audio tensor to a NumPy array and play it using the IPython Audio display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "audio_np = audio.numpy()\n",
    "\n",
    "audio_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Audio(audio_np,rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y = audio_np,\n",
    "        mode = 'lines',\n",
    "        name = 'Audio'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title = 'Time',\n",
    "    yaxis_title = 'Amplitude',\n",
    "    title = 'Audio waveform',\n",
    "    width=800,\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze the spectrogram, which provides a time-frequency representation of audio.\n",
    "A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "spectrogram = librosa.stft(audio_np, n_fft= 1024)\n",
    "spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n",
    "\n",
    "time = np.linspace(0, len(audio_np)/16000,spectrogram_db.shape[1])\n",
    "frequency = np.linspace(0, 16000/2, spectrogram_db.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z = spectrogram_db,\n",
    "            x = time,\n",
    "            y = frequency,\n",
    "            #colorscale = 'viridis',\n",
    "            colorbar=dict(title='Amplitude (dB)')\n",
    "\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Spectrogram\",\n",
    "    xaxis_title=\"Time (seconds)\",\n",
    "    yaxis_title=\"Frequency (Hz)\",\n",
    "    yaxis = dict(type= 'log'),\n",
    "    template = 'plotly_dark'\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_family_counts = {instrument_families[family_id]: count \n",
    "                        for family_id, count in instrument_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy\n",
    "\n",
    "instrument_counts = Counter()\n",
    "\n",
    "for sample in dataset.take(1000):\n",
    "  instrument = sample['instrument']['family'].numpy()\n",
    "  instrument_counts[instrument] += 1\n",
    "\n",
    "instrument_families = [\"Bass\", \"Brass\", \"Flute\", \"Guitar\", \"Keyboard\", \"Mallet\", \"Organ\", \"Reed\", \"String\", \"Synth Lead\", \"Synth Pad\", \"Vocal\"]\n",
    "\n",
    "'''x = instrument_families[family_id]\n",
    "print(x) '''\n",
    "\n",
    "mapped_family_counts = {instrument_families[family_id]: count \n",
    "                        for family_id, count in instrument_counts.items()}\n",
    "print(mapped_family_counts)\n",
    "\n",
    "\n",
    "''' mapped_family_counts = {instrument_families[family_id] : count \n",
    "                        for family_id,count in instrument_counts.items()}  '''   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    x = list(mapped_family_counts.keys()),\n",
    "    y = list(mapped_family_counts.values()),\n",
    "    title='instrument family distrubution'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title = 'instrument family',\n",
    "    yaxis_title = 'count of instruments'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze the Mel spectrogram, which translates audio frequencies into the Mel scale, to simulate human perception of sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = librosa.feature.melspectrogram(y=audio_np, sr=16000, n_fft=1024, hop_length=512)\n",
    "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram)\n",
    "mel_spectrogram_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    go.Heatmap(\n",
    "        z = mel_spectrogram_db,\n",
    "        x = time,\n",
    "        y = frequency,\n",
    "        colorbar=dict(title=\"Amplitude (dB)\")\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title = 'mel spectrogram',\n",
    "    xaxis_title = 'time',\n",
    "    yaxis_title = 'frequency',\n",
    "    template = 'plotly_dark'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=audio_np,sr=16000,n_mfcc=13)\n",
    "mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    go.Heatmap(\n",
    "        z = mfcc,\n",
    "        x = time,\n",
    "        y= frequency,\n",
    "        colorbar=dict(title='mfcc value')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title = 'mfcc',\n",
    "    xaxis_title = 'time',\n",
    "    yaxis_title = 'frequency',\n",
    "    template = 'plotly_dark'\n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_pitch_shifted = librosa.effects.pitch_shift(audio_np, sr=16000, n_steps=2)\n",
    "\n",
    "audio_time_stretched = librosa.effects.time_stretch(audio_np, rate=1.5)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=audio_np, mode='lines', name='Original'))\n",
    "fig.add_trace(go.Scatter(y=audio_pitch_shifted, mode='lines', name='Pitch Shifted'))\n",
    "fig.add_trace(go.Scatter(y=audio_time_stretched, mode='lines', name='Time Stretched'))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
